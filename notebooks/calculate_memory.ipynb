{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import time\n",
    "from robustness import datasets\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "\n",
    "from qtorch.quant import Quantizer, quantizer\n",
    "from qtorch.optim import OptimLP\n",
    "from torch.optim import SGD\n",
    "from qtorch import FloatingPoint\n",
    "from qtorch.auto_low import sequential_lower\n",
    "import math\n",
    "\n",
    "import itertools\n",
    "import utils\n",
    "import train_func as tf\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# low precision configurations\n",
    "low_formats = [['Representation_Float', '5', 1],['Representation_Float', '6', 2],\n",
    "               ['Representation_Float', '7', 3],['Representation_Float', '8', 4],\n",
    "               ['Representation_Float', '6', 1],['Representation_Float', '7', 2],\n",
    "               ['Representation_Float', '8', 3],['Representation_Float', '9', 4],\n",
    "               ['Representation_Float', '7', 1],['Representation_Float', '8', 2],\n",
    "               ['Representation_Float', '9', 3]]\n",
    "\n",
    "high_formats = [['Representation_Float', '14', 7],['Representation_Float', '16', 9],\n",
    "                ['Representation_Float', '18', 11],['Representation_Float', '15', 7],\n",
    "                ['Representation_Float', '17', 9],['Representation_Float', '19', 11],\n",
    "                ['Representation_Float', '16', 7],['Representation_Float', '18', 9],\n",
    "                ['Representation_Float', '20', 11]]\n",
    "\n",
    "lp_configs = list(itertools.product(low_formats,high_formats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_matrix = np.zeros((88, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Start Evaluating on CIFAR10\")\n",
    "\n",
    "resolution = (3, 32, 32)\n",
    "batch_size = 32\n",
    "arch = 'resnet18'\n",
    "data = 'cifar10'\n",
    "epo = 10\n",
    "bs = 32\n",
    "lr = 0.001\n",
    "transform = 'default'\n",
    "data_dir = './data/'\n",
    "\n",
    "transforms = tf.load_transforms(transform)\n",
    "trainset = tf.load_trainset(data, transforms, path=data_dir)\n",
    "print(\"Number of classes in {} is: {}\".format(data,trainset.num_classes))\n",
    "\n",
    "for i in range(len(lp_configs)):\n",
    "    current_config = lp_configs[i]\n",
    "    print(\"Currently trains on : {}\".format(current_config))\n",
    "    low_format,high_format = current_config[0],current_config[1]\n",
    "    man_low,man_high = low_format[2],high_format[2]\n",
    "    exp_low = int(low_format[1])-man_low-1\n",
    "    exp_high = int(high_format[1])-man_high-1\n",
    "    bit_low = FloatingPoint(exp=exp_low, man=man_low)\n",
    "    bit_high = FloatingPoint(exp=exp_high, man=man_high)\n",
    "    low_quant_func = lambda: Quantizer(forward_number=bit_low, backward_number=bit_low,\n",
    "                            forward_rounding=\"stochastic\", backward_rounding=\"stochastic\")\n",
    "    high_quant = Quantizer(forward_number=bit_high, backward_number=bit_high,\n",
    "                            forward_rounding=\"stochastic\", backward_rounding=\"stochastic\")\n",
    "    ## load model\n",
    "    net = tf.load_architectures(arch, low_quant_func, high_quant, trainset.num_classes)\n",
    "    mem_matrix[0,i] = summary(net, resolution, batch_size, LP=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Start Evaluating on CIFAR100\")\n",
    "\n",
    "resolution = (3, 32, 32)\n",
    "batch_size = 32\n",
    "arch = 'resnet18'\n",
    "data = 'cifar10'\n",
    "epo = 10\n",
    "bs = 32\n",
    "lr = 0.001\n",
    "transform = 'default'\n",
    "data_dir = './data/'\n",
    "\n",
    "for idx in range(20):\n",
    "    for i in range(len(lp_configs)):\n",
    "        current_config = lp_configs[i]\n",
    "        print(\"Currently trains on : {}\".format(current_config))\n",
    "        low_format,high_format = current_config[0],current_config[1]\n",
    "        man_low,man_high = low_format[2],high_format[2]\n",
    "        exp_low = int(low_format[1])-man_low-1\n",
    "        exp_high = int(high_format[1])-man_high-1\n",
    "        bit_low = FloatingPoint(exp=exp_low, man=man_low)\n",
    "        bit_high = FloatingPoint(exp=exp_high, man=man_high)\n",
    "        low_quant_func = lambda: Quantizer(forward_number=bit_low, backward_number=bit_low,\n",
    "                                forward_rounding=\"stochastic\", backward_rounding=\"stochastic\")\n",
    "        high_quant = Quantizer(forward_number=bit_high, backward_number=bit_high,\n",
    "                                forward_rounding=\"stochastic\", backward_rounding=\"stochastic\")\n",
    "        ## load model\n",
    "        net = tf.load_architectures(arch, low_quant_func, high_quant, 5)\n",
    "        mem_matrix[idx+1,i] = summary(net, resolution, batch_size, LP=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ds_dir = \"datasets/my_deca\"\n",
    "resolution = (3, 48, 48)\n",
    "dataset_list = os.listdir(ds_dir)\n",
    "for ds_id, ds_name in enumerate(dataset_list):\n",
    "    test_transforms = tf.load_transforms('deca_test')\n",
    "    testset = tf.load_trainset(ds_name, test_transforms, train=False, path='datasets/my_deca')\n",
    "    for i in range(len(lp_configs)):\n",
    "        current_config = lp_configs[i]\n",
    "        print(\"Currently trains on : {}\".format(current_config))\n",
    "        low_format,high_format = current_config[0],current_config[1]\n",
    "        man_low,man_high = low_format[2],high_format[2]\n",
    "        exp_low = int(low_format[1])-man_low-1\n",
    "        exp_high = int(high_format[1])-man_high-1\n",
    "        bit_low = FloatingPoint(exp=exp_low, man=man_low)\n",
    "        bit_high = FloatingPoint(exp=exp_high, man=man_high)\n",
    "        low_quant_func = lambda: Quantizer(forward_number=bit_low, backward_number=bit_low,\n",
    "                                forward_rounding=\"stochastic\", backward_rounding=\"stochastic\")\n",
    "        high_quant = Quantizer(forward_number=bit_high, backward_number=bit_high,\n",
    "                                forward_rounding=\"stochastic\", backward_rounding=\"stochastic\")\n",
    "        config = \"l_{}_{}_h_{}_{}\".format(low_format[1],low_format[2],high_format[1],high_format[2])\n",
    "        ## load model     \n",
    "        net = tf.load_architectures(arch, low_quant_func, high_quant, testset.num_classes)\n",
    "        mem_matrix[21+ds_id,i] = summary(net, resolution, batch_size, LP=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start Evaluating on imagenet\")\n",
    "base_model_dir_list = [\"saved_models/imagenet_subsets/lp_resnet18+\",\"_epo10_bs32_lr0.001_mom0.9_wd0.0005\"]\n",
    "\n",
    "in_path = 'dir/to/imagenet64x64\n",
    "in_info_path = 'dir/to/imagenet64x64'\n",
    "\n",
    "from robustness.tools.imagenet_helpers import ImageNetHierarchy\n",
    "in_hier = ImageNetHierarchy(in_path,\n",
    "                            in_info_path)\n",
    "\n",
    "superclasses_list = []\n",
    "for cnt, (wnid, ndesc_in, ndesc_total) in enumerate(reversed(in_hier.wnid_sorted)):\n",
    "    if ndesc_in >= 5:\n",
    "        #print(f\"WordNet ID: {wnid}, Name: {in_hier.wnid_to_name[wnid]}, #ImageNet descendants: {ndesc_in}\")\n",
    "        superclasses_list.append(wnid)\n",
    "#NEED TO CHANGE\n",
    "subclass_id_dict = set()\n",
    "count = 0\n",
    "used = 0\n",
    "start = 0\n",
    "i = 0\n",
    "superclass_ids = []\n",
    "class_ranges_list = []\n",
    "superclass_names = []\n",
    "#label_map_list = []\n",
    "\n",
    "while i < 50:\n",
    "    ancestor_wnid = superclasses_list[count]\n",
    "    # print(f\"Superclass | WordNet ID: {ancestor_wnid}, Name: {in_hier.wnid_to_name[ancestor_wnid]}\")\n",
    "    class_ranges, _ = in_hier.get_subclasses([ancestor_wnid],balanced=True)\n",
    "    class_ranges_no_dup = set()\n",
    "    for idx, class_id_imgnet in enumerate(class_ranges[0]):\n",
    "        if not (class_id_imgnet in subclass_id_dict):\n",
    "            class_ranges_no_dup.add(class_id_imgnet)\n",
    "            subclass_id_dict.add(class_id_imgnet)\n",
    "    label = len(class_ranges_no_dup)\n",
    "    class_ranges = [{list(class_ranges_no_dup)[i]} for i in range(len(class_ranges_no_dup))]\n",
    "    if label > 4:\n",
    "        used += 1\n",
    "    if label > 4 and used > start:\n",
    "        superclass_names.append(in_hier.wnid_to_name[ancestor_wnid])\n",
    "        superclass_ids.append(ancestor_wnid)\n",
    "        class_ranges_list.append(class_ranges)\n",
    "        i += 1\n",
    "    count += 1 \n",
    "\n",
    "for id in range(50):\n",
    "    print(\"Evaluating on Superclass {} with WordNetid {}\".format(superclass_names[id],superclass_ids[id]))\n",
    "    class_ranges = class_ranges_list[id]\n",
    "    for i in range(len(lp_configs)):\n",
    "        current_config = lp_configs[i]\n",
    "        print(\"Currently trains on : {}\".format(current_config))\n",
    "        low_format,high_format = current_config[0],current_config[1]\n",
    "        man_low,man_high = low_format[2],high_format[2]\n",
    "        exp_low = int(low_format[1])-man_low-1\n",
    "        exp_high = int(high_format[1])-man_high-1\n",
    "        bit_low = FloatingPoint(exp=exp_low, man=man_low)\n",
    "        bit_high = FloatingPoint(exp=exp_high, man=man_high)\n",
    "        low_quant_func = lambda: Quantizer(forward_number=bit_low, backward_number=bit_low,\n",
    "                                forward_rounding=\"stochastic\", backward_rounding=\"stochastic\")\n",
    "        high_quant = Quantizer(forward_number=bit_high, backward_number=bit_high,\n",
    "                                forward_rounding=\"stochastic\", backward_rounding=\"stochastic\")\n",
    "        config = \"l_{}_{}_h_{}_{}\".format(low_format[1],low_format[2],high_format[1],high_format[2])\n",
    "        ## load model\n",
    "        net = tf.load_architectures(arch, low_quant_func, high_quant, len(class_ranges))\n",
    "        mem_matrix[38+id,i] = summary(net, resolution, batch_size, LP=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"mem_matrix_full\", mem_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
